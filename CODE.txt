/*********************************************************************************

                       #2.1 Simple Linear Regressioin

*********************************************************************************/

Demonstrate Simple Linear Regression model using R
# Load the mtcars dataset
data(mtcars)
# Inspect the structure of the dataset
str(mtcars)
# Print the first few rows of the dataset
head(mtcars)
# Check for missing values in the dataset
sum(is.na(mtcars))
# Build the Simple Linear Regression model
model <- lm(mpg ~ wt, data = mtcars)
# Print the summary of the model
summary(model)
# Make predictions using the model
new_data <- data.frame(wt = c(2.5, 3.0, 3.5))
predictions <- predict(model, newdata = new_data)
# Scatterplot with predicted values
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  geom_point(data = data.frame(wt = new_data$wt, mpg = predictions),
             aes(x = wt, y = mpg), color = "blue") +
  labs(x = "Weight", y = "Miles per Gallon", title = "Scatterplot of mpg vs. wt with Predictions")

/*********************************************************************************

                       #2.1 (Alernate Code) Simple Linear Regressioin

*********************************************************************************/

# The predictor vector.
x <- c(154, 171, 128, 180, 130, 135, 174, 123, 159, 132)
# The resposne vector.
y <- c(76, 72, 62, 48, 63, 81, 56, 91, 47, 57)
# Apply the lm() function.
relation <- lm(y~x)
print(relation)
print(summary(relation))
# Find weight of a person with height 170.
a <- data.frame(x = 170)
result <-  predict(relation,a)
print(result)
# Plot the chart.
plot(y,x,col = "blue",main = "Height & Weight Regression",abline(lm(x~y)),cex = 1.3,pch = 16,xlab = "Weight in Kg",ylab = "Height in cm")

/*********************************************************************************

                       #2.2 Multiple Linear Regressioin

*********************************************************************************/

# load dataset
data(mtcars)

# check for missing values
sum(is.na(mtcars))

# check for outliers
boxplot(mtcars)

# scale variables (if necessary)
scaled_mtcars <- scale(mtcars)

# prepare model
model <- lm(mpg ~ wt + hp + cyl, data = mtcars)
summary(model)

# predict mpg of a car
new_data <- data.frame(wt = 3.2, hp = 200, cyl = 8)
predict(model, newdata = new_data)

# evaluate model
summary(model)


/*********************************************************************************

                       #3 Logistic Regressioin

*********************************************************************************/


data(mtcars)

mpg_median <- median(mtcars$mpg)
mtcars$efficiency <- ifelse(mtcars$mpg >= mpg_median, "high", "low")
mtcars$efficiency <- ifelse(mtcars$mpg >= mpg_median, 1, 0)
mtcars$am <- as.factor(mtcars$am)

model <- glm(efficiency ~ cyl + hp + wt + am, data = mtcars, family = "binomial")

new_car <- data.frame(cyl = 6, hp = 115, wt = 2.9, am = 1)
new_car$am <- factor(new_car$am, levels = levels(mtcars$am))

# Corrected line for prediction
prediction <- predict(model, newdata = new_car, type = "response")
prediction <- ifelse(prediction >= 0.5, 1, 0)

summary(model)

predictions <- factor(prediction, levels = c(0, 1))
mtcars$efficiency <- factor(mtcars$efficiency, levels = c(0, 1))


library(ggplot2)
coef_df <- data.frame(coef = coef(model)[-1], variable = names(coef(model)[-1]))
ggplot(coef_df, aes(x = variable, y = coef)) + 
  geom_bar(stat = "identity", fill = "blue", width = 0.5) + 
  coord_flip() + 
  ggtitle("Coefficients of Logistic Regression Model")

/*********************************************************************************

                       #3 Logistic Regressioin.py

*********************************************************************************/
import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv("insurance_data.csv")
df.head()
plt.scatter(df.age,df.bought_insurance,marker='+',color='red')

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test =train_test_split(df[['age']],df.bought_insurance,train_size=0.8)
X_test
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
X_test

y_predicted = model.predict(X_test)
print(y_predicted)

model.predict_proba(X_test)
print(model.predict_proba(X_test))

model.score(X_test,y_test)
print(model.score(X_test,y_test))

y_predicted
X_test
print(model.coef_)
print(model.intercept_)

import math
def sigmoid(x):
    return 1 / (1 + math.exp(-x))
def prediction_function(age):
    z = 0.042 * age - 1.53 # 0.04150133 ~ 0.042 and -1.52726963 ~ -1.53
    y = sigmoid(z)
    return y

def Check(age):
    op=prediction_function(age)
    if op<0.5:
        print(op,"is less than 0.5 which means person with",age,"age will not buy insurance")
    else:
        print(op,"is greater than 0.5 which means person with",age,"age will buy insurance")

Check(35)
Check(50)


/*********************************************************************************

                       #4 Hypothesis Testing 

*********************************************************************************/


# Load the dataset
data <- ChickWeight

# Perform the t-test
t.test(data$weight, mu=160, alternative="greater")
# Load the datasets
data1 <- PlantGrowth[PlantGrowth$group == "ctrl",]$weight
data2 <- PlantGrowth[PlantGrowth$group == "trt1",]$weight

# Perform the t-test
t.test(data1, data2)
# Load the dataset
data <- ToothGrowth

# Perform the t-test
t.test(data$len ~ data$supp, paired=TRUE)

data <- ChickWeight

# Perform the ANOVA
model <- lm(weight ~ Diet, data=data)
anova(model)


/*********************************************************************************
                       #5 Titanic 

*********************************************************************************/

sum(is.na(data))

# Replace missing values with 0
data[is.na(data)] <- 0

# Replace missing values with the mean of the variable
mean_age <- mean(data$Age, na.rm = TRUE)
data$Age[is.na(data$Age)] <- mean_age

# Install and load the mice package
install.packages("mice")
library(mice)

# Impute missing values with the mean of the variable
imp_data <- mice(data, method = "mean")

# Extract the completed data
completed_data <- complete(imp_data)

# Install and load the caret package
install.packages("caret")
library(caret)
# Convert the Sex variable to dummy variables
dummies <- dummyVars(~ Sex, data = data)
dummy_data <- predict(dummies, newdata = data)

# Create boxplots to identify outliers
boxplot(data$Age)

# Remove outliers
data <- data[data$Age < 80, ]
plot(data) 


/*********************************************************************************

                       #7  Decision Tree 

*********************************************************************************/

# Install required libraries
install.packages("rpart") 
install.packages("tree")
install.packages("rattle")

# Load required libraries
library(rpart)
library(rpart.plot)

# Load the iris dataset
data(iris)

# Split data into training and testing sets
set.seed(123)
train_index <- sample(1:nrow(iris), 100)
train_data <- iris[train_index, ]
test_data <- iris[-train_index, ]

# Build Decision Tree model
dt_model <- rpart(Species ~ ., data = train_data, method = "class")

# Visualize the Decision Tree
rpart.plot(dt_model)

# Make predictions on the testing set
predictions <- predict(dt_model, test_data, type = "class")

# Evaluate model accuracy
accuracy <- sum(predictions == test_data$Species) / nrow(test_data)
print(paste("Accuracy: ", round(accuracy * 100, 2), "%"))



/*********************************************************************************

                       #8 PCA

*********************************************************************************/

# Loading the iris dataset
data(iris)

# Scaling the dataset
scaled_data <- scale(iris[, 1:4])
pca <- prcomp(scaled_data, center = TRUE, scale. = TRUE)
# Getting the summary of the PCA
summary(pca)
# Plotting the PCA results
# Biplot
biplot(pca, scale = 0.2)
# Scatter plot
plot(pca$x[, 1], pca$x[, 2], col = iris$Species, pch = 19)
# Heatmap
 heatmap(cor(scaled_data), xlab = "", ylab = "", main = "Correlation Matrix Heatmap")


/*********************************************************************************

                       #9 Time Series

*********************************************************************************/

install.packages('forecast')
library('forecast')
class(AirPassengers)
AirPassengers
plot(AirPassengers)
data<-ts(AirPassengers, frequency=12)
d<-decompose(data, "multiplicative")
plot(d)
model<-auto.arima(AirPassengers)
# h = 10*12 because, forecast is for 10 years for all 12 months
f<-forecast(model, level=c(95), h=10*12)
plot(f)



/*********************************************************************************

                       #10 Clustering K-Means

*********************************************************************************/


# Load the iris dataset
data(iris)

# Choose the features to be used for clustering
features <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width")

# Normalize the features
iris_scaled <- scale(iris[, features])

# Determine the optimal number of clusters using the elbow method
sse <- numeric(10)
for (k in 1:10) {
  km <- kmeans(iris_scaled, k)
  sse[k] <- km$tot.withinss
}
plot(1:10, sse, type="b", xlab="Number of clusters", ylab="Sum of squared errors")

From the elbow plot, it seems that the optimal value of K is 3. We can now run K-means clustering with K=3 and visualize the clusters:
# Run K-means clustering with K=3
km <- kmeans(iris_scaled, 3)

# Visualize the clusters
library(ggplot2)
iris_clustered <- data.frame(iris, cluster = as.factor(km$cluster))
ggplot(iris_clustered, aes(Petal.Length, Petal.Width, color=cluster)) + geom_point()

